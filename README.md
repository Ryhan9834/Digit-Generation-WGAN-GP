
# ğŸ§  Digit Generation using WGAN-GP

This project implements a **Wasserstein GAN with Gradient Penalty (WGAN-GP)** to generate realistic handwritten digits from the MNIST dataset. WGAN-GP addresses several key issues of traditional GANs â€” such as mode collapse and training instability â€” by introducing a more principled loss function and enforcing a smoothness constraint through gradient penalty.

---

## ğŸ¯ Objectives

- âœ… Implement a stable and robust GAN variant: **WGAN-GP**
- ğŸ§  Train on the MNIST dataset to generate synthetic digits
- ğŸ“‰ Track training progress using **Wasserstein loss**
- ğŸ–¼ Apply **Gradient Penalty (GP)** to enforce the 1-L constraint on the critic
- ğŸ” Visualize generated samples per epoch

---

## ğŸ›  Tech Stack

- **Frameworks**: PyTorch
- **Data**: MNIST (28x28 grayscale)
- **Core Concepts**: Generator, Critic, W-Loss, Gradient Penalty
- **Tools**: `tqdm`, `matplotlib` for tracking & visualization

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ models/
â”‚ â””â”€â”€ generator.pth            # Trained generator weights
â”‚ â””â”€â”€ critic.pth               # Trained critic weights
â”œâ”€â”€ WGAN_with_GP.ipynb         # Jupyter notebook with full code
â”œâ”€â”€ generated_images/          # Output images from steps
â”œâ”€â”€ generated_images.gif       # GIF with the Output images
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ README.md                  # This file
```

---

## ğŸ“ˆ Results

- Smooth training curves using W-Loss
- No mode collapse
- Moderate-quality digit generation

---

## ğŸš€ How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/your-username/WGAN-GP-Digits.git
   cd WGAN-GP-Digits
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Launch the notebook:
   ```bash
   jupyter notebook WGAN_with_GP.ipynb
   ```

4. Or load the pretrained generator:
   ```python
   from load_generator import load_generator
   gen = load_generator()
   ```

---

## ğŸ“¸ Sample Outputs

| Step 5000 | Step 10000 | Step 15000 |
|----------|----------|----------|
| ![](generated_images/step_5000.png) | ![](generated_images/step_10000.png) | ![](generated_images/step_15000.png) |

---

## ğŸ“‰ Training Loss Curve

<img src="loss_curve.png" alt="Loss Curve" width="700" height="500"/>

---

## ğŸ§  Why WGAN-GP?

> Traditional GANs suffer from vanishing gradients and unstable training. WGAN-GP introduces the **Wasserstein loss** and a **gradient penalty** to enforce a Lipschitz constraint, resulting in more stable and meaningful training.

---

## ğŸ“š References

- [Wasserstein GAN with Gradient Penalty (arXiv)](https://arxiv.org/abs/1704.00028)
- [PyTorch Docs](https://pytorch.org/)
- [Build Better GANs (Coursera)](https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans)

---

## ğŸ† Author

Made with â¤ï¸ by [Md. Ryhan Uddin](https://github.com/Ryhan9834)
